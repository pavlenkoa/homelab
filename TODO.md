# TODO

## Current Status: OrbStack Foundation - INFRASTRUCTURE MIGRATION âœ…

**âœ… Phases 1-2 COMPLETED** (Foundation + GitOps)
- **Kubernetes Cluster**: OrbStack native Kubernetes (switched from K3d)
- **GitOps Platform**: ArgoCD with enterprise-grade App-of-Apps pattern
- **Ingress Controller**: ingress-nginx v4.12.0 (NodePort 30080/30443)
- **Certificate Management**: cert-manager v1.16.2 with Let's Encrypt
- **GitOps Management**: ArgoCD UI at http://localhost:30081 (admin/BQUb-QTq3RkNkNUm)
- **Repository**: Public GitHub integration with automated sync
- **Template System**: Extensible App-of-Apps with global defaults and per-app overrides

**ðŸ”„ INFRASTRUCTURE CHANGES:**
- **Cilium Removed**: Not supported on OrbStack, switched to standard networking
- **ingress-nginx Added**: External access via NodePort (http://localhost:30080, https://localhost:30443)
- **cert-manager Added**: Automated TLS certificate management with Let's Encrypt
- **OrbStack Benefits**: Native macOS integration, better performance, simpler networking

**ðŸ”§ IMMEDIATE PRIORITIES:**
1. **Deploy cert-manager and ingress-nginx** via ArgoCD App-of-Apps
2. **Configure Let's Encrypt ClusterIssuer** for automatic TLS certificates
3. **Test external access** through ingress-nginx NodePort
4. **Proceed with Phase 3** - Platform services deployment

**ðŸš€ READY: Phase 3** - Deploy platform services (Vault, VictoriaMetrics, Grafana) with proper ingress

---

## OrbStack Kubernetes Migration Plan

### Phase 1: Kubernetes Foundation Setup âœ… COMPLETED
- [x] **Enable OrbStack Kubernetes cluster**:
  - [x] Configure OrbStack Kubernetes via UI (one-click setup)
  - [x] Verify cluster connectivity: `kubectl get nodes`
  - [x] Install Helm package manager
  - [x] Configure kubectl context for homelab cluster (renamed to 'homelab')
- [x] **Deploy ingress-nginx and cert-manager** (replaced Cilium - not supported on OrbStack):
  - [x] Create App-of-Apps configuration for cert-manager v1.16.2
  - [x] Create App-of-Apps configuration for ingress-nginx v4.12.0
  - [x] Create `kubernetes/infrastructure/values/homelab/cert-manager.yaml` values
  - [x] Create `kubernetes/infrastructure/values/homelab/ingress-nginx.yaml` values
  - [x] Configure NodePort access (30080/30443) for external connectivity
  - [x] Enable Prometheus metrics for monitoring integration

### Phase 2: ArgoCD GitOps Setup âœ… COMPLETED
- [x] **Deploy ArgoCD to Kubernetes cluster**:
  - [x] Install ArgoCD via Helm with NodePort access (http://localhost:30081)
  - [x] Configure insecure mode for local development
  - [x] Set up admin access: admin / BQUb-QTq3RkNkNUm
  - [x] Connect ArgoCD to https://github.com/pavlenkoa/homelab.git repository (public)
- [x] **Create App-of-Apps Helm Chart**:
  - [x] Create `kubernetes/app-of-apps/Chart.yaml` 
  - [x] Create `kubernetes/app-of-apps/templates/_application.tpl` helper with global defaults
  - [x] Create `kubernetes/app-of-apps/templates/applications.yaml` generator
  - [x] Create `kubernetes/app-of-apps/values.yaml` with global defaults
  - [x] Create `kubernetes/app-of-apps/values/homelab.yaml` with service definitions
  - [x] Deploy app-of-apps with extensible template system
  - [x] Verify GitOps workflow: Git push â†’ ArgoCD auto-sync â†’ Cluster update
  - [x] **Enhanced App-of-Apps Features**:
    - [x] Global defaults with per-application overrides
    - [x] Support for multiple Helm value files and inline values
    - [x] Configurable sync policies, destinations, and repositories
    - [x] Enterprise-grade template system for scalability

### Phase 3: Platform Services - Core Infrastructure
- [ ] **Deploy Caddy Ingress Controller**:
  - [ ] Create `kubernetes/infrastructure/charts/caddy/` chart 
  - [ ] Create `kubernetes/infrastructure/values/homelab/caddy.yaml` values
  - [ ] Configure auto-TLS with Let's Encrypt
  - [ ] Set up ingress rules for all services
  - [ ] Verify external access: `https://homelab.domain.com`
- [ ] **Deploy HashiCorp Vault**:
  - [ ] Create `kubernetes/platform/charts/vault/` chart using HashiCorp Helm chart
  - [ ] Create `kubernetes/platform/values/homelab/vault.yaml` values  
  - [ ] Install Vault: `helm install vault hashicorp/vault`
  - [ ] Initialize and unseal Vault cluster: `kubectl exec vault-0 -- vault operator init`
  - [ ] Configure secret engines (KV v2, PKI, Transit)
  - [ ] Set up Vault UI access via Caddy ingress
- [ ] **Deploy External Secrets Operator**:
  - [ ] Create `kubernetes/platform/charts/external-secrets/` chart
  - [ ] Create `kubernetes/platform/values/homelab/external-secrets.yaml` values
  - [ ] Install ESO: `helm install external-secrets external-secrets/external-secrets`
  - [ ] Configure Vault SecretStore and ClusterSecretStore
  - [ ] Test secret synchronization from Vault to K8s secrets

### Phase 4: Platform Services - Monitoring & Authentication
- [ ] **Deploy VictoriaMetrics Stack**:
  - [ ] Create `kubernetes/platform/charts/victoriametrics/` chart using VM Helm charts
  - [ ] Create `kubernetes/platform/values/homelab/victoriametrics.yaml` values
  - [ ] Install VictoriaMetrics: `helm install vm vm/victoria-metrics-cluster`
  - [ ] Configure persistent storage for metrics data
- [ ] **Deploy VictoriaLogs**:
  - [ ] Create `kubernetes/platform/charts/victorialogs/` chart
  - [ ] Create `kubernetes/platform/values/homelab/victorialogs.yaml` values
  - [ ] Install VictoriaLogs: `helm install vlogs vm/victoria-logs-single`
  - [ ] Configure log retention and storage policies
- [ ] **Deploy Grafana**:
  - [ ] Create `kubernetes/platform/charts/grafana/` chart using Grafana Helm chart
  - [ ] Create `kubernetes/platform/values/homelab/grafana.yaml` values
  - [ ] Install Grafana: `helm install grafana grafana/grafana`
  - [ ] Configure VictoriaMetrics and VictoriaLogs data sources
  - [ ] Import monitoring dashboards from current Docker setup
  - [ ] Set up alerting rules and notification channels
- [ ] **Deploy Alloy (Metrics Collection)**:
  - [ ] Create `kubernetes/platform/charts/alloy/` chart using Grafana Helm chart
  - [ ] Create `kubernetes/platform/values/homelab/alloy.yaml` values
  - [ ] Install Alloy: `helm install alloy grafana/alloy`
  - [ ] Configure metrics collection from Kubernetes and external sources
  - [ ] Set up metrics forwarding to VictoriaMetrics
- [ ] **Deploy Authelia**:
  - [ ] Create `kubernetes/platform/charts/authelia/` custom chart
  - [ ] Create `kubernetes/platform/values/homelab/authelia.yaml` values
  - [ ] Configure Redis StatefulSet for session storage
  - [ ] Migrate authentication configuration from Docker Compose
  - [ ] Set up OIDC integration with other services
  - [ ] Configure Caddy integration for SSO

### Phase 5: Application Services Migration
- [ ] **Deploy Immich (Photo Management)**:
  - [ ] Create `kubernetes/applications/charts/immich/` chart
  - [ ] Create `kubernetes/applications/values/homelab/immich.yaml` values
  - [ ] Configure PostgreSQL StatefulSet with persistent storage
  - [ ] Configure Redis for caching and job queues
  - [ ] Set up persistent volume for photo uploads
  - [ ] Configure NFS connection to Kyiv media storage
  - [ ] Migrate OAuth configuration from Docker Compose
  - [ ] Test photo upload and external library scanning
- [ ] **Deploy Emby (Media Server)**:
  - [ ] Create `kubernetes/applications/charts/emby/` chart  
  - [ ] Create `kubernetes/applications/values/homelab/emby.yaml` values
  - [ ] Configure persistent storage for Emby data
  - [ ] Set up NFS mounts for media access from Kyiv
  - [ ] Configure hardware transcoding (if supported)
  - [ ] Test media streaming and transcoding functionality

### Phase 6: Advanced Platform Features  
- [ ] **Deploy Hubble UI (Network Observability)**:
  - [ ] Create `kubernetes/platform/charts/hubble-ui/` chart
  - [ ] Create `kubernetes/platform/values/homelab/hubble-ui.yaml` values
  - [ ] Install Hubble UI: `helm install hubble-ui cilium/hubble-ui`
  - [ ] Configure network flow visualization and monitoring
- [ ] **Deploy HashiCorp Consul (Optional)**:
  - [ ] Create `kubernetes/platform/charts/consul/` chart using HashiCorp Helm chart
  - [ ] Create `kubernetes/platform/values/homelab/consul.yaml` values
  - [ ] Install Consul: `helm install consul hashicorp/consul`
  - [ ] Configure service mesh and service discovery
  - [ ] Set up Consul Connect for service-to-service encryption
- [ ] **Deploy HashiCorp Boundary (Optional)**:
  - [ ] Create `kubernetes/platform/charts/boundary/` chart
  - [ ] Create `kubernetes/platform/values/homelab/boundary.yaml` values
  - [ ] Configure secure remote access and session recording
  - [ ] Integrate with Vault for dynamic credentials

### Phase 7: Hybrid Architecture (Kubernetes + Docker)
- [ ] **Kyiv Raspberry Pi integration**:
  - [ ] Keep Transmission on Docker (storage-local BitTorrent)
  - [ ] Update Alloy configuration to collect metrics from Kyiv Docker services
  - [ ] Configure NFS exports for Kubernetes persistent volumes
  - [ ] Set up secure tunneling for cross-location service communication
- [ ] **Service placement optimization**:
  - [ ] CPU-intensive workloads â†’ Mac Mini Kubernetes (Emby transcoding, VictoriaMetrics)
  - [ ] Storage-heavy services â†’ Kyiv Docker/NFS (Transmission, media storage)
  - [ ] Control plane and management â†’ Kubernetes (ArgoCD, Grafana, Vault)
  - [ ] Cross-location monitoring â†’ Kubernetes Alloy agents â†’ Kyiv Docker metrics

### Phase 8: GitOps Workflow Optimization
- [ ] **Complete ArgoCD GitOps Integration**:
  - [ ] Ensure all services deployed via ArgoCD App-of-Apps pattern
  - [ ] Configure automatic sync policies and health checks
  - [ ] Set up ArgoCD notifications and alerting
  - [ ] Test rollback and disaster recovery procedures
- [ ] **Environment Management**:
  - [ ] Create staging environment values (`kubernetes/*/values/staging/`)
  - [ ] Implement environment promotion workflows
  - [ ] Set up branch-based deployment strategies
- [ ] **Monitoring and Alerting Integration**:
  - [ ] Configure Grafana alerting for Kubernetes cluster health
  - [ ] Set up alerts for ArgoCD sync failures and application health
  - [ ] Create runbooks for common operational tasks

### Phase 9: Migration Strategy and Testing
- [ ] **Parallel deployment testing**:
  - [ ] Run services in both Docker and Kubernetes
  - [ ] Compare performance and resource usage
  - [ ] Test failover and disaster recovery scenarios
  - [ ] Validate all functionality matches Docker deployment
- [ ] **Gradual migration approach**:
  - [ ] Start with stateless services (Caddy, Authelia)
  - [ ] Move monitoring stack (already stateless design)
  - [ ] Migrate stateful services last (Immich with data)
  - [ ] Keep critical services in Docker until K8s proven stable
- [ ] **Documentation and runbooks**:
  - [ ] Update ARCHITECTURE.md for hybrid K8s/Docker model
  - [ ] Create operational runbooks for common tasks
  - [ ] Document troubleshooting procedures
  - [ ] Update deployment workflows and commands

## Kubernetes Directory Structure

### Final Structure Decision: Centralized Values Pattern
Based on proven enterprise patterns and professional experience transfer, using centralized structure similar to established company practices:

```
homelab/
â”œâ”€â”€ docker-compose/              # Current Docker setup (preserved)
â”‚   â”œâ”€â”€ _base/
â”‚   â”œâ”€â”€ wroclaw/
â”‚   â”œâ”€â”€ kyiv/
â”‚   â””â”€â”€ scripts/                 # Docker Compose automation
â”‚       â”œâ”€â”€ deploy               # Main deployment CLI
â”‚       â”œâ”€â”€ Makefile             # Make command wrapper
â”‚       â”œâ”€â”€ template.py          # Jinja2 templating
â”‚       â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ kubernetes/                  # New Kubernetes manifests
â”‚   â”œâ”€â”€ infrastructure/          # Core cluster infrastructure
â”‚   â”‚   â”œâ”€â”€ charts/             # Helm charts for infrastructure
â”‚   â”‚   â”‚   â”œâ”€â”€ cilium/         # CNI with Hubble
â”‚   â”‚   â”‚   â””â”€â”€ caddy/          # Ingress controller with auto-TLS
â”‚   â”‚   â”œâ”€â”€ templates/          # Jinja2 templates (optional, complex cases)
â”‚   â”‚   â”‚   â”œâ”€â”€ caddy.yaml.j2
â”‚   â”‚   â”‚   â””â”€â”€ cilium.yaml.j2
â”‚   â”‚   â””â”€â”€ values/             # Environment-specific values
â”‚   â”‚       â””â”€â”€ homelab/        # Primary environment
â”‚   â”‚           â”œâ”€â”€ cilium.yaml
â”‚   â”‚           â”œâ”€â”€ cert-manager.yaml # Optional if using Caddy auto-TLS
â”‚   â”‚           â””â”€â”€ caddy.yaml
â”‚   â”œâ”€â”€ platform/               # Platform services
â”‚   â”‚   â”œâ”€â”€ charts/
â”‚   â”‚   â”‚   â”œâ”€â”€ vault/          # Secret management
â”‚   â”‚   â”‚   â”œâ”€â”€ external-secrets/ # Vault integration
â”‚   â”‚   â”‚   â”œâ”€â”€ consul/         # Service mesh and discovery
â”‚   â”‚   â”‚   â”œâ”€â”€ authelia/       # Authentication
â”‚   â”‚   â”‚   â”œâ”€â”€ grafana/        # Visualisation & Notifications (new grafana includes extensive alerting)
â”‚   â”‚   â”‚   â”œâ”€â”€ victoriametrics/ # Metrics storage
â”‚   â”‚   â”‚   â”œâ”€â”€ victorialogs/   # Logs storage
â”‚   â”‚   â”‚   â”œâ”€â”€ alloy/          # Metrics collection agent
â”‚   â”‚   â”‚   â”œâ”€â”€ hubble-ui/      # Cilium network observability
â”‚   â”‚   â”‚   â””â”€â”€ boundary/       # Secure access (optional)
â”‚   â”‚   â”œâ”€â”€ templates/          # Platform-specific Jinja2 templates
â”‚   â”‚   â””â”€â”€ values/
â”‚   â”‚       â””â”€â”€ homelab/
â”‚   â”‚           â”œâ”€â”€ vault.yaml
â”‚   â”‚           â”œâ”€â”€ external-secrets.yaml
â”‚   â”‚           â”œâ”€â”€ consul.yaml
â”‚   â”‚           â”œâ”€â”€ authelia.yaml
â”‚   â”‚           â”œâ”€â”€ grafana.yaml
â”‚   â”‚           â”œâ”€â”€ victoriametrics.yaml
â”‚   â”‚           â”œâ”€â”€ victorialogs.yaml
â”‚   â”‚           â”œâ”€â”€ alloy.yaml
â”‚   â”‚           â”œâ”€â”€ hubble-ui.yaml
â”‚   â”‚           â””â”€â”€ boundary.yaml
â”‚   â”œâ”€â”€ applications/           # Business applications
â”‚   â”‚   â”œâ”€â”€ charts/
â”‚   â”‚   â”‚   â”œâ”€â”€ immich/         # Photo management
â”‚   â”‚   â”‚   â””â”€â”€ emby/           # Media server
â”‚   â”‚   â”œâ”€â”€ templates/          # Application-specific templates
â”‚   â”‚   â””â”€â”€ values/
â”‚   â”‚       â””â”€â”€ homelab/
â”‚   â”‚           â”œâ”€â”€ immich.yaml
â”‚   â”‚           â””â”€â”€ emby.yaml
â”‚   â”œâ”€â”€ app-of-apps/            # ArgoCD App of Apps pattern
â”‚   â”‚   â”œâ”€â”€ Chart.yaml          # Helm chart for generating ArgoCD apps
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â”œâ”€â”€ _application.tpl # Template helper for ArgoCD apps
â”‚   â”‚   â”‚   â””â”€â”€ applications.yaml # Dynamic ArgoCD app generation
â”‚   â”‚   â””â”€â”€ values/
â”‚   â”‚       â””â”€â”€ homelab.yaml    # Defines which apps to deploy
â”‚   â””â”€â”€ scripts/                # Templating and deployment automation
â”‚       â”œâ”€â”€ template.py         # Jinja2 templating script
â”‚       â””â”€â”€ deploy.py           # Environment deployment script
â””â”€â”€ docs/                       # Documentation (preserved)
```

### Benefits of This Structure
- **Professional alignment**: Matches enterprise patterns for skill transfer
- **Environment-first thinking**: Easy to see all services for an environment
- **Templating power**: Jinja2 generates values from infrastructure state
- **Scalable**: Can add staging/dev environments by adding new directories
- **Clear separation**: Charts, templates, and values cleanly separated
- **ArgoCD native**: App-of-apps pattern with Helm chart generation

### Templating Strategy
- **Optional Jinja2**: Use templates only for complex cases requiring cross-service values
- **Static values**: Most services use simple YAML values files
- **Environment generation**: Script can template entire environment from infrastructure state
- **Flexible granularity**: Template per service, per environment, or everything

### ArgoCD Application Structure
- **App-of-Apps Helm Chart**: Generates ArgoCD applications dynamically
- **Three-layer deployment**: Infrastructure â†’ Platform â†’ Applications
- **Environment-aware**: All apps point to correct environment values directory

### Success Criteria
- [ ] **Zero-downtime migration**: Services remain available during transition
- [ ] **Feature parity**: All current functionality preserved in Kubernetes
- [ ] **Improved scalability**: Better resource utilization on Mac Mini M4
- [ ] **Enhanced security**: Vault-managed secrets, service mesh, network policies
- [ ] **GitOps workflow**: All changes deployed through git commits
- [ ] **Monitoring excellence**: Better observability than current Docker setup
- [ ] **Disaster recovery**: Complete infrastructure as code in git

### Risk Mitigation
- [ ] **Rollback plan**: Ability to quickly return to Docker Compose
- [ ] **Resource monitoring**: Ensure 16GB RAM sufficient for Kubernetes overhead
- [ ] **Data backup**: Full backup before migration starts
- [ ] **Service isolation**: Prevent single component failure from affecting others
- [ ] **Gradual approach**: Phase-by-phase migration with validation at each step

---

## Current Status: Docker Compose Architecture âœ… COMPLETED

The current Docker Compose infrastructure is fully operational with:
- âœ… 2-location setup (WrocÅ‚aw Mac Mini + Kyiv Raspberry Pi)
- âœ… All services migrated to docker-compose/ directory structure
- âœ… Professional CLI with service selection workflow
- âœ… Template processing and secret management
- âœ… GitOps-ready configuration management
- âœ… Comprehensive monitoring with VictoriaLogs + VictoriaMetrics

**Ready for Kubernetes migration while maintaining full Docker Compose fallback capability.**
